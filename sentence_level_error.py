# -*- coding: utf-8 -*-
"""Sentence Level Error

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QlBuDLhIXBPgrsw-zd0DAeq_fL3kiiO9
"""

"""
Comprehensive Data Quality Error Detection for African Language NLP Tasks

This script detects data quality issues in both token-based (NER, POS) and
sentence-based (MT, QA) tasks using explainability methods.

Requirements:
pip install torch transformers datasets captum numpy pandas scipy scikit-learn
"""

import torch
import numpy as np
from typing import Dict, List, Tuple, Optional, Union
from dataclasses import dataclass
from enum import Enum
import warnings
warnings.filterwarnings('ignore')

# Explainability imports
from captum.attr import IntegratedGradients, LayerIntegratedGradients
from transformers import AutoTokenizer, AutoModel, AutoModelForTokenClassification
from transformers import AutoModelForSeq2SeqLM, AutoModelForQuestionAnswering
from scipy.spatial.distance import cosine
from sklearn.cluster import DBSCAN
import pandas as pd


class TaskType(Enum):
    """Supported task types"""
    NER = "ner"
    POS = "pos"
    SENTIMENT = "sentiment"
    MT = "machine_translation"
    QA = "question_answering"


@dataclass
class ErrorReport:
    """Structure for storing detected errors"""
    error_type: str
    confidence: float
    evidence: Dict
    example_id: str
    details: str
    recommendations: str


class DataQualityChecker:
    """Main class for detecting data quality issues"""

    def __init__(self, model_name: str, task_type: TaskType, device: str = "cuda"):
        """
        Initialize the checker with a model and task type

        Args:
            model_name: HuggingFace model name or path
            task_type: Type of NLP task
            device: Device to run on (cuda/cpu)
        """
        self.task_type = task_type
        self.device = device if torch.cuda.is_available() else "cpu"
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)

        # Load appropriate model based on task
        if task_type in [TaskType.NER, TaskType.POS]:
            self.model = AutoModelForTokenClassification.from_pretrained(model_name)
        elif task_type == TaskType.MT:
            self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
        elif task_type == TaskType.QA:
            self.model = AutoModelForQuestionAnswering.from_pretrained(model_name)
        else:
            self.model = AutoModel.from_pretrained(model_name)

        self.model.to(self.device)
        self.model.eval()

        # Initialize explainability tools
        self.integrated_gradients = IntegratedGradients(self.model)

        print(f"âœ“ Initialized {task_type.value} checker with {model_name}")

    def check_all_errors(self, dataset: List[Dict]) -> List[ErrorReport]:
        """
        Check for all relevant errors based on task type

        Args:
            dataset: List of examples with required fields based on task

        Returns:
            List of ErrorReport objects
        """
        all_errors = []

        if self.task_type in [TaskType.NER, TaskType.POS]:
            all_errors.extend(self._check_token_based_errors(dataset))
        elif self.task_type == TaskType.MT:
            all_errors.extend(self._check_mt_errors(dataset))
        elif self.task_type == TaskType.QA:
            all_errors.extend(self._check_qa_errors(dataset))
        elif self.task_type == TaskType.SENTIMENT:
            all_errors.extend(self._check_sentiment_errors(dataset))

        return all_errors

    # ============================================================================
    # TOKEN-BASED TASK ERROR DETECTION (NER, POS)
    # ============================================================================

    def _check_token_based_errors(self, dataset: List[Dict]) -> List[ErrorReport]:
        """Check errors for token classification tasks"""
        errors = []

        print("Checking annotation inconsistencies...")
        errors.extend(self._detect_annotation_inconsistencies(dataset))

        print("Checking label noise...")
        errors.extend(self._detect_label_noise(dataset))

        print("Checking spurious correlations...")
        errors.extend(self._detect_spurious_correlations_tokens(dataset))

        return errors

    def _detect_annotation_inconsistencies(self, dataset: List[Dict]) -> List[ErrorReport]:
        """
        Detect cases where similar tokens have different labels

        Expected format: {"tokens": [...], "labels": [...], "id": "..."}
        """
        errors = []
        token_label_map = {}  # Track all labels for each token

        # Build token-label mapping
        for example in dataset:
            tokens = example.get("tokens", [])
            labels = example.get("labels", [])

            for token, label in zip(tokens, labels):
                token_lower = token.lower()
                if token_lower not in token_label_map:
                    token_label_map[token_lower] = []
                token_label_map[token_lower].append({
                    "label": label,
                    "example_id": example.get("id", "unknown"),
                    "context": " ".join(tokens)
                })

        # Find inconsistencies
        for token, label_instances in token_label_map.items():
            unique_labels = set(inst["label"] for inst in label_instances)

            if len(unique_labels) > 1 and len(label_instances) >= 3:
                # Significant inconsistency found
                label_counts = {}
                for inst in label_instances:
                    label_counts[inst["label"]] = label_counts.get(inst["label"], 0) + 1

                confidence = 1.0 - (min(label_counts.values()) / sum(label_counts.values()))

                errors.append(ErrorReport(
                    error_type="annotation_inconsistency",
                    confidence=confidence,
                    evidence={
                        "token": token,
                        "label_distribution": label_counts,
                        "examples": label_instances[:5]
                    },
                    example_id="multiple",
                    details=f"Token '{token}' has {len(unique_labels)} different labels: {unique_labels}",
                    recommendations="Review annotation guidelines for this entity type"
                ))

        return errors

    def _detect_label_noise(self, dataset: List[Dict]) -> List[ErrorReport]:
        """
        Detect cases where model's 'wrong' prediction might actually be correct
        Uses attention and attribution to validate model's reasoning
        """
        errors = []

        for example in dataset[:100]:  # Sample to avoid long processing
            tokens = example.get("tokens", [])
            true_labels = example.get("labels", [])
            example_id = example.get("id", "unknown")

            # Get model predictions
            inputs = self.tokenizer(
                tokens,
                is_split_into_words=True,
                return_tensors="pt",
                truncation=True
            ).to(self.device)

            with torch.no_grad():
                outputs = self.model(**inputs)
                predictions = torch.argmax(outputs.logits, dim=-1)[0]

            # Get attention weights
            with torch.no_grad():
                outputs_with_attn = self.model(**inputs, output_attentions=True)
                attention = outputs_with_attn.attentions[-1][0].mean(dim=0)  # Average over heads

            # Find disagreements between prediction and true label
            word_ids = inputs.word_ids()
            for idx, word_id in enumerate(word_ids):
                if word_id is None:
                    continue

                if word_id < len(true_labels):
                    pred_label = predictions[idx].item()
                    true_label = true_labels[word_id] if isinstance(true_labels[word_id], int) else 0

                    if pred_label != true_label:
                        # Check if model has strong evidence for its prediction
                        token_attention = attention[idx].cpu().numpy()
                        attention_confidence = float(token_attention.max())

                        if attention_confidence > 0.3:  # High confidence in model's choice
                            errors.append(ErrorReport(
                                error_type="potential_label_noise",
                                confidence=attention_confidence,
                                evidence={
                                    "token": tokens[word_id],
                                    "true_label": true_label,
                                    "predicted_label": pred_label,
                                    "attention_confidence": attention_confidence,
                                    "context": " ".join(tokens)
                                },
                                example_id=example_id,
                                details=f"Model strongly predicts {pred_label} but label is {true_label}",
                                recommendations="Manual review: model might be correct"
                            ))

        return errors

    def _detect_spurious_correlations_tokens(self, dataset: List[Dict]) -> List[ErrorReport]:
        """
        Detect if model relies on position or format cues rather than content
        """
        errors = []
        position_label_map = {}  # Track labels by position in sentence

        for example in dataset:
            tokens = example.get("tokens", [])
            labels = example.get("labels", [])

            for pos, label in enumerate(labels):
                relative_pos = pos / max(len(labels), 1)  # Normalize position
                pos_bucket = int(relative_pos * 10)  # Bucket into 10 positions

                if pos_bucket not in position_label_map:
                    position_label_map[pos_bucket] = []
                position_label_map[pos_bucket].append(label)

        # Check for suspicious position-label correlations
        for pos_bucket, labels in position_label_map.items():
            if len(labels) > 20:  # Enough samples
                label_counts = {}
                for label in labels:
                    label_counts[label] = label_counts.get(label, 0) + 1

                max_label = max(label_counts, key=label_counts.get)
                max_ratio = label_counts[max_label] / len(labels)

                if max_ratio > 0.7:  # 70% of labels at this position are the same
                    errors.append(ErrorReport(
                        error_type="spurious_position_correlation",
                        confidence=max_ratio,
                        evidence={
                            "position_bucket": pos_bucket,
                            "dominant_label": max_label,
                            "ratio": max_ratio,
                            "label_distribution": label_counts
                        },
                        example_id="multiple",
                        details=f"Label {max_label} appears in {max_ratio:.1%} of position {pos_bucket}",
                        recommendations="Check if position-based artifact exists in data"
                    ))

        return errors

    # ============================================================================
    # MACHINE TRANSLATION ERROR DETECTION
    # ============================================================================

    def _check_mt_errors(self, dataset: List[Dict]) -> List[ErrorReport]:
        """Check errors specific to machine translation"""
        errors = []

        print("Checking semantic drift...")
        errors.extend(self._detect_semantic_drift(dataset))

        print("Checking translationese...")
        errors.extend(self._detect_translationese(dataset))

        print("Checking code-switching inconsistency...")
        errors.extend(self._detect_code_switching(dataset))

        return errors

    def _detect_semantic_drift(self, dataset: List[Dict]) -> List[ErrorReport]:
        """
        Detect translation pairs with semantic misalignment

        Expected format: {"source": "...", "target": "...", "id": "..."}
        """
        errors = []

        for example in dataset[:100]:
            source = example.get("source", "")
            target = example.get("target", "")
            example_id = example.get("id", "unknown")

            if not source or not target:
                continue

            # Encode source and target
            source_inputs = self.tokenizer(
                source,
                return_tensors="pt",
                truncation=True,
                max_length=128
            ).to(self.device)

            target_inputs = self.tokenizer(
                target,
                return_tensors="pt",
                truncation=True,
                max_length=128
            ).to(self.device)

            with torch.no_grad():
                # Get encoder outputs for cross-attention analysis
                encoder_outputs = self.model.get_encoder()(**source_inputs)

                # Generate with cross-attention
                outputs = self.model.generate(
                    **source_inputs,
                    max_length=128,
                    return_dict_in_generate=True,
                    output_attentions=True
                )

                # Check cross-attention patterns
                if hasattr(outputs, 'cross_attentions') and outputs.cross_attentions:
                    cross_attn = outputs.cross_attentions[0][0].mean(dim=1)  # Average over heads

                    # Calculate attention entropy (uniform = high entropy = poor alignment)
                    attn_probs = cross_attn.mean(dim=0).cpu().numpy()
                    attn_probs = attn_probs / (attn_probs.sum() + 1e-9)
                    entropy = -np.sum(attn_probs * np.log(attn_probs + 1e-9))
                    max_entropy = np.log(len(attn_probs))
                    normalized_entropy = entropy / max_entropy

                    if normalized_entropy > 0.8:  # Very uniform attention
                        errors.append(ErrorReport(
                            error_type="semantic_drift",
                            confidence=normalized_entropy,
                            evidence={
                                "source": source,
                                "target": target,
                                "attention_entropy": float(normalized_entropy),
                                "interpretation": "Cross-attention is very scattered"
                            },
                            example_id=example_id,
                            details="Translation pair may have semantic misalignment",
                            recommendations="Verify source and target convey same meaning"
                        ))

        return errors

    def _detect_translationese(self, dataset: List[Dict]) -> List[ErrorReport]:
        """
        Detect unnatural target language (word-for-word translation)
        """
        errors = []

        # Check for rigid word-order alignment
        for example in dataset[:50]:
            source = example.get("source", "")
            target = example.get("target", "")
            example_id = example.get("id", "unknown")

            if not source or not target:
                continue

            source_tokens = source.split()
            target_tokens = target.split()

            # Simple heuristic: very similar length might indicate word-for-word
            length_ratio = len(target_tokens) / max(len(source_tokens), 1)

            if 0.9 <= length_ratio <= 1.1 and len(source_tokens) > 5:
                # Check cross-attention for rigid alignment
                source_inputs = self.tokenizer(
                    source,
                    return_tensors="pt",
                    truncation=True
                ).to(self.device)

                with torch.no_grad():
                    outputs = self.model.generate(
                        **source_inputs,
                        max_length=128,
                        return_dict_in_generate=True,
                        output_attentions=True
                    )

                    if hasattr(outputs, 'cross_attentions') and outputs.cross_attentions:
                        # Check if attention follows diagonal pattern (word-for-word)
                        cross_attn = outputs.cross_attentions[0][0].mean(dim=1)
                        attn_matrix = cross_attn.cpu().numpy()

                        # Calculate diagonal dominance
                        min_dim = min(attn_matrix.shape)
                        diagonal_sum = sum(attn_matrix[i, i] for i in range(min_dim))
                        total_sum = attn_matrix.sum()
                        diagonal_ratio = diagonal_sum / (total_sum + 1e-9)

                        if diagonal_ratio > 0.4:  # Strong diagonal pattern
                            errors.append(ErrorReport(
                                error_type="translationese",
                                confidence=diagonal_ratio,
                                evidence={
                                    "source": source,
                                    "target": target,
                                    "length_ratio": length_ratio,
                                    "diagonal_ratio": float(diagonal_ratio)
                                },
                                example_id=example_id,
                                details="Translation follows source word order too rigidly",
                                recommendations="Check if target language sounds natural"
                            ))

        return errors

    def _detect_code_switching(self, dataset: List[Dict]) -> List[ErrorReport]:
        """
        Detect inconsistent handling of code-switched content
        """
        errors = []
        code_switch_patterns = {}

        # Look for potential English words in source (simple heuristic)
        for example in dataset:
            source = example.get("source", "")
            target = example.get("target", "")

            source_words = source.split()
            target_words = target.split()

            # Simple English word detection (can be improved)
            for s_word in source_words:
                # Check if word appears in target (copied) or translated
                if s_word.lower() in [t.lower() for t in target_words]:
                    action = "copied"
                else:
                    action = "translated"

                if s_word not in code_switch_patterns:
                    code_switch_patterns[s_word] = []
                code_switch_patterns[s_word].append(action)

        # Find inconsistent patterns
        for word, actions in code_switch_patterns.items():
            if len(actions) >= 3:
                copied_count = actions.count("copied")
                translated_count = actions.count("translated")

                if copied_count > 0 and translated_count > 0:
                    inconsistency_rate = min(copied_count, translated_count) / len(actions)

                    if inconsistency_rate > 0.2:  # 20% inconsistency
                        errors.append(ErrorReport(
                            error_type="code_switching_inconsistency",
                            confidence=inconsistency_rate,
                            evidence={
                                "word": word,
                                "copied_count": copied_count,
                                "translated_count": translated_count,
                                "total_occurrences": len(actions)
                            },
                            example_id="multiple",
                            details=f"Word '{word}' is sometimes copied, sometimes translated",
                            recommendations="Establish consistent code-switching guidelines"
                        ))

        return errors

    # ============================================================================
    # QUESTION ANSWERING ERROR DETECTION
    # ============================================================================

    def _check_qa_errors(self, dataset: List[Dict]) -> List[ErrorReport]:
        """Check errors specific to question answering"""
        errors = []

        print("Checking question-answer misalignment...")
        errors.extend(self._detect_qa_misalignment(dataset))

        print("Checking context contradictions...")
        errors.extend(self._detect_context_contradiction(dataset))

        print("Checking answer length bias...")
        errors.extend(self._detect_answer_length_bias(dataset))

        print("Checking template artifacts...")
        errors.extend(self._detect_template_artifacts(dataset))

        return errors

    def _detect_qa_misalignment(self, dataset: List[Dict]) -> List[ErrorReport]:
        """
        Detect questions and answers that don't match in type

        Expected format: {"question": "...", "context": "...", "answer": "...", "id": "..."}
        """
        errors = []

        question_words = {
            "who": ["person", "name", "people"],
            "when": ["time", "date", "year", "day"],
            "where": ["place", "location", "country", "city"],
            "how many": ["number", "count"],
            "what": ["thing", "object", "event"],
            "why": ["reason", "because", "cause"]
        }

        for example in dataset[:100]:
            question = example.get("question", "").lower()
            answer = example.get("answer", "")
            example_id = example.get("id", "unknown")

            # Detect question type
            question_type = None
            for q_word, expected_answer_types in question_words.items():
                if question.startswith(q_word) or f" {q_word} " in question:
                    question_type = q_word
                    break

            if question_type:
                # Simple heuristic checks
                misalignment_detected = False
                reason = ""

                if question_type == "who" and answer.isdigit():
                    misalignment_detected = True
                    reason = "Who question but answer is a number"
                elif question_type == "when" and len(answer.split()) > 10:
                    misalignment_detected = True
                    reason = "When question but answer is very long (not a date/time)"
                elif question_type == "how many" and not any(char.isdigit() for char in answer):
                    misalignment_detected = True
                    reason = "How many question but answer contains no numbers"

                if misalignment_detected:
                    errors.append(ErrorReport(
                        error_type="qa_misalignment",
                        confidence=0.7,
                        evidence={
                            "question": question,
                            "answer": answer,
                            "question_type": question_type,
                            "reason": reason
                        },
                        example_id=example_id,
                        details=reason,
                        recommendations="Verify answer matches question type"
                    ))

        return errors

    def _detect_context_contradiction(self, dataset: List[Dict]) -> List[ErrorReport]:
        """
        Detect answers that contradict the context
        """
        errors = []

        for example in dataset[:50]:
            question = example.get("question", "")
            context = example.get("context", "")
            answer = example.get("answer", "")
            example_id = example.get("id", "unknown")

            if not all([question, context, answer]):
                continue

            # Check if answer appears in context
            if answer.lower() not in context.lower():
                # Use model to find what it would predict
                inputs = self.tokenizer(
                    question,
                    context,
                    return_tensors="pt",
                    truncation=True,
                    max_length=384
                ).to(self.device)

                with torch.no_grad():
                    outputs = self.model(**inputs)
                    start_scores = outputs.start_logits[0]
                    end_scores = outputs.end_logits[0]

                    start_idx = torch.argmax(start_scores).item()
                    end_idx = torch.argmax(end_scores).item()

                    if end_idx >= start_idx:
                        tokens = self.tokenizer.convert_ids_to_tokens(
                            inputs.input_ids[0][start_idx:end_idx+1]
                        )
                        model_answer = self.tokenizer.convert_tokens_to_string(tokens)

                        # Model found different answer in context
                        if model_answer.strip() and model_answer.lower() != answer.lower():
                            errors.append(ErrorReport(
                                error_type="context_contradiction",
                                confidence=0.8,
                                evidence={
                                    "question": question,
                                    "gold_answer": answer,
                                    "model_answer": model_answer,
                                    "context_snippet": context[:200]
                                },
                                example_id=example_id,
                                details="Gold answer not found in context, model predicts different answer",
                                recommendations="Check if annotator used external knowledge"
                            ))

        return errors

    def _detect_answer_length_bias(self, dataset: List[Dict]) -> List[ErrorReport]:
        """
        Detect systematic bias in answer lengths
        """
        errors = []
        answer_lengths = []

        for example in dataset:
            answer = example.get("answer", "")
            answer_lengths.append(len(answer.split()))

        if len(answer_lengths) > 20:
            mean_length = np.mean(answer_lengths)
            std_length = np.std(answer_lengths)

            # Check if most answers are very similar length
            within_one_std = sum(1 for l in answer_lengths
                                if abs(l - mean_length) <= std_length)
            ratio = within_one_std / len(answer_lengths)

            if ratio > 0.85 and std_length < 2.0:  # 85% within 1 std and low variance
                errors.append(ErrorReport(
                    error_type="answer_length_bias",
                    confidence=ratio,
                    evidence={
                        "mean_length": float(mean_length),
                        "std_length": float(std_length),
                        "concentration_ratio": ratio
                    },
                    example_id="multiple",
                    details=f"85% of answers have length within {std_length:.1f} words of mean ({mean_length:.1f})",
                    recommendations="Check if answer span selection has length bias"
                ))

        return errors

    def _detect_template_artifacts(self, dataset: List[Dict]) -> List[ErrorReport]:
        """
        Detect templated question generation patterns
        """
        errors = []
        question_structures = {}

        for example in dataset:
            question = example.get("question", "")

            # Extract structure (first 3 and last 2 words)
            words = question.split()
            if len(words) > 5:
                structure = " ".join(words[:3]) + " ... " + " ".join(words[-2:])

                if structure not in question_structures:
                    question_structures[structure] = []
                question_structures[structure].append(example.get("id", "unknown"))

        # Find over-represented structures
        total_questions = len(dataset)
        for structure, example_ids in question_structures.items():
            if len(example_ids) > 0.1 * total_questions:  # >10% of questions
                errors.append(ErrorReport(
                    error_type="template_artifacts",
                    confidence=len(example_ids) / total_questions,
                    evidence={
                        "template_structure": structure,
                        "occurrence_count": len(example_ids),
                        "percentage": f"{100 * len(example_ids) / total_questions:.1f}%"
                    },
                    example_id="multiple",
                    details=f"Question structure '{structure}' appears in {len(example_ids)} examples",
                    recommendations="Check for synthetic question generation artifacts"
                ))

        return errors

    # ============================================================================
    # SENTIMENT ANALYSIS ERROR DETECTION
    # ============================================================================

    def _check_sentiment_errors(self, dataset: List[Dict]) -> List[ErrorReport]:
        """Check errors specific to sentiment analysis"""
        errors = []

        print("Checking label noise in sentiment...")
        errors.extend(self._detect_sentiment_label_noise(dataset))

        print("Checking class imbalance...")
        errors.extend(self._detect_class_imbalance(dataset))

        return errors

    def _detect_sentiment_label_noise(self, dataset: List[Dict]) -> List[ErrorReport]:
        """
        Detect sentiment labels that might be wrong (especially sarcasm)

        Expected format: {"text": "...", "label": 0/1/2, "id": "..."}
        """
        errors = []

        # Sarcasm indicators (simple heuristic)
        sarcasm_indicators = ["great job", "wonderful", "excellent", "amazing"]
        negative_words = ["destroy", "ruin", "disaster", "terrible", "worst"]

        for example in dataset[:100]:
            text = example.get("text", "").lower()
            label = example.get("label", 1)  # Assuming 0=neg, 1=neu, 2=pos
            example_id = example.get("id", "unknown")

            # Check for sarcasm pattern: positive words + negative context
            has_positive_words = any(word in text for word in sarcasm_indicators)
            has_negative_words = any(word in text for word in negative_words)

            if has_positive_words and has_negative_words and label == 2:  # Labeled as positive
                errors.append(ErrorReport(
                    error_type="sentiment_label_noise",
                    confidence=0.7,
                    evidence={
                        "text": text,
                        "label": label,
                        "detected_pattern": "Possible sarcasm"
                    },
                    example_id=example_id,
                    details="Text contains positive and negative words, might be sarcastic",
                    recommendations="Manual review for sarcasm detection"
                ))

        return errors

    def _detect_class_imbalance(self, dataset: List[Dict]) -> List[ErrorReport]:
        """
        Detect severe class imbalance in classification tasks
        """
        errors = []
        label_counts = {}

        for example in dataset:
            label = example.get("label", 1)
            label_counts[label] = label_counts.get(label, 0) + 1

        if len(label_counts) > 1:
            total = sum(label_counts.values())
            max_class = max(label_counts, key=label_counts.get)
            max_ratio = label_counts[max_class] / total

            if max_ratio > 0.7:  # 70% imbalance
                errors.append(ErrorReport(
                    error_type="class_imbalance",
                    confidence=max_ratio,
                    evidence={
                        "label_distribution": label_counts,
                        "majority_class": max_class,
                        "majority_ratio": max_ratio
                    },
                    example_id="multiple",
                    details=f"Class {max_class} represents {max_ratio:.1%} of data",
                    recommendations="Consider data augmentation or weighted loss"
                ))

        return errors

    # ============================================================================
    # REPORTING AND EXPORT
    # ============================================================================

    def generate_report(self, errors: List[ErrorReport], output_file: str = "data_quality_report.txt"):
        """Generate human-readable report"""

        # Group errors by type
        errors_by_type = {}
        for error in errors:
            if error.error_type not in errors_by_type:
                errors_by_type[error.error_type] = []
            errors_by_type[error.error_type].append(error)

        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n